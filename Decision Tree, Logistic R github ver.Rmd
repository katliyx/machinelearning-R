---
title: "Assignment-4"
author: "Karen Gao, Katherine Li, Qing Luan"
date: "12/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PART I: Collect the Data

### Q1 Load the data
```{r,message=FALSE}

library(readr)
mushroom <- read_csv("https://s3.amazonaws.com/notredame.analytics.data/mushrooms.csv")

```

### Q2 Replace the type feature with a new boolean feature called edible with values "Yes" and "No".
```{r}

library(dplyr)

names(mushroom)[names(mushroom) == "type"] <- "edible"
mushroom <- mushroom %>%
  mutate(edible = recode(edible, "poisonous" = "No")) %>%
  mutate(edible = recode(edible, "edible" = "Yes"))

```


## PART II: Explore and Prepare the Data

### Q1 Using statistical and graphical metohds, explore the data. Reduce the dimensionality of the dataset to only features that are likely to be useful in improving the performance of the model. 

First, using the graphical method, display the data distributions for all features. After picking out the features that obviously have values clustered under one class, we decide to check on features that include gill_attachment, gill_spacing, ring_number, veil_color and veil_type, and conduct further inspection and examination. (Please zoom in to see the details of the graph below.)
```{r}

# Change the data types for data of character type to factor. 
mushroom <- mushroom %>%
  mutate_if(is.character,as.factor)

library(tidyverse)

# Plot the data distributions for all features. 
mushroom %>%
  keep(is.factor) %>%
  gather() %>%
  group_by(key,value) %>% 
  summarise(n = n()) %>% 
  ggplot() +
  geom_bar(mapping=aes(x = value, y = n, fill=key), color="black", stat='identity') + 
  coord_flip() +
  facet_wrap(~ key, scales = "free") +
  theme_minimal()

```

Second, by using the statistical method, check the data distributions for the selected features from the previous step by computing the exact percentages that each classs takes up. If a certain class occupies a much higher percentage than the other class(es) under the same feature, it indicates that the feature has a low information value. For instance, for feature gill_attachment here, the ones labeled as "free" take up around 97.42%, compared with other classes under this feature. Thus, get rid of this feature in future endeavor. Another example would be that for feature ring_number, those who have one ring dominates over other classes under this feature, taking up roughly 92.17%. Following this logic, we get rid of the features that have low information values, including gill_attachment, gill_spacing, ring_number, veil_color and veil_type.
```{r}

# Compute the data distribution of the selected features. 
round(prop.table(table(select(mushroom,gill_attachment))),4) * 100
round(prop.table(table(select(mushroom,gill_spacing))),4) * 100
round(prop.table(table(select(mushroom,ring_number))),4) * 100
round(prop.table(table(select(mushroom,veil_color))),4) * 100
round(prop.table(table(select(mushroom,veil_type))),4) * 100

# Get rid of the features. 
mushroom <- select(mushroom, -gill_attachment, -gill_spacing, -ring_number, -veil_color, -veil_type)

```


### Q2 Using a stratified sampling approach, split the data into training and test datasets by the ratio of 60:40. Then, display the class distribution for the original dataset, training dataset, and test dataset, respectively. We could see that the datasets are all sufficiently balanced, and are ready to be used.
```{r}

# Split the data into training and test datasets. 
set.seed(1234)
sample_set <- sample(nrow(mushroom), round(nrow(mushroom)*.60), replace = FALSE)
mushroom_train <- mushroom[sample_set, ]
mushroom_test <- mushroom[-sample_set, ]

# Display the class distribution for the datasets.
round(prop.table(table(select(mushroom, edible), exclude = NULL)), 4) * 100
round(prop.table(table(select(mushroom_train , edible), exclude = NULL)), 4) * 100
round(prop.table(table(select(mushroom_test, edible), exclude = NULL)), 4) * 100

```

## PART III: Train the Models
Out of the three supervised learning techniques that we have covered, we choose to use Decision Trees and Logistic Regression for this case. We first decide on using the Decision Tree method because it could help us generate a powerful classifier. It is known for the ability to perform well on most problems; and it is useful for both larger and small datasets. The typical output of the resulting tree structure is easy to understand and inspect. And very importantly, as decision tree models handle nominal features well, it would be a good choice for this dataset. Additionally, it also ignores unimportant features - this is especially efficient for us since this dataset includes quite a lot of features, and it would be time-consuming if the model needs to go over all the features without selection. Overall, using Decision Tree here would be efficient and of low cost. 

As for the Logistic Regression method, since it is a probabilistic statistical regression model which is used to model the relationship between predictor variables and the odds of a categorical response. It especially works well for this case as our expected outcomes would be either yes or no. Also, it is known for being very efficient to train, and being easy to implement and use. Since the prompt asks us to use only one independent variable, and logistic regression model could handle a reasonable number of nominal features - the nature of this case would not be a problem to prevent us using it. 

We do not use the k-Nearest Neighbor method because all the features in our dataset are categorical. Since the mechanism of this algorithm uses Euclidean distance to locate the nearest neighbor(s), applying the k-NN method in this case would require a lot of data processing. Plus, for this particular dataset, even with the help of the dummies R package, it is still utterly difficult to transform all classes of features. Therefore, by nature, this dataset does not work well with K-NN.

### Decision Trees Model
Set the pre-pruning complexity parameter to 0.001. As per the prompt requested, no post-pruning is needed. 
```{r}

library(rpart)
library(rpart.plot)

# Train the decision tree model. 
tree_mod <-
  rpart(
    edible ~ .,
    method = "class",
    data = mushroom_train,
    control = rpart.control(cp = 0.001))

# Plot the decision tree using the rpart.plot() function from the rpart.plot library.
rpart.plot(tree_mod)

```

### Logistic Regression Model
As requested, we use only one independent variable and set the cut-off at 0.5. Through the hint from the prompt saying that "it's important to note that, because of their nature, greedy learners are sometimes useful in feature selection and can be used to inform the input of other more sophisticated learners", we select feature odor here from the output of the decision tree above. It is the feature that is the first split is about, so it should be the most relevant variable - since the decision about which feature to split is usually made with the goal of maximizing purity (while minimizing impurity). Low impurity implies large homogeneity within each side after the split. For the next step, the decision tree algorithm would calculate information gains from the splits on possible features. And the split with the highest information gain is chosen. So here, since the first split is on feature odor, it is associated with the highest information gain. Thus, we choose feature odor as the independent variable for our logistic regression. 
```{r}

# Using the glm() function from the stats package, build the logistic regression model.
logit_mod <-
  glm(edible ~ odor, family = binomial(link = 'logit'), data = mushroom_train)

# View the results of the model.
summary(logit_mod)

```

## PART IV: Evaluate the Performance of the Models
### Q1. Using each of the models trained in the previous section, predict whether the mushroom samples in the test dataset are edible or not.
### Decision Trees Model
```{r}

# Make predictions using our tree model against the test set.
tree_pred <- predict(tree_mod, mushroom_test,  type = "class")
head(tree_pred)

# Exclude type="class" to get predicted probabilities.
tree_pred_prob <- predict(tree_mod, mushroom_test)
head(tree_pred_prob)

```

### Logistic Regression Model
```{r}

# Generate predictions against the data using the model. Cut-off set at 0.5
logit_pred <- predict(logit_mod, mushroom_test, type = 'response')
logit_pred <- ifelse(logit_pred > 0.5, 'Yes', 'No')
head(logit_pred)

```

### Q2. Create a confusion matrix of the predictions against actuals, and display the accuracy of each model against the test data.
### Decision Trees Model
```{r}

# Using our predictions, we can construct the Confusion Matrix.
tree_pred_table <- table(mushroom_test$edible, tree_pred)
tree_pred_table

# Compute the accuracy of the model against the test data. 
tree_pred_accuracy <- sum(diag(tree_pred_table)) / nrow(mushroom_test)
tree_pred_accuracy

```

### Logistic Regression Model
```{r}

# Using our predictions, construct a Confusion Matrix.
logit_pred_table <- table(mushroom_test$edible, logit_pred)
logit_pred_table

# Compute the accuracy of the model against the test data. 
sum(diag(logit_pred_table)) / nrow(mushroom_test)

```



